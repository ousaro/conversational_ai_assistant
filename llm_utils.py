from langchain_ollama import OllamaLLM
from env import CHAT_MODEL
from typing import Union, List, Dict, Iterator

def invoke_llm(prompt: Union[str, List[Dict]], model: str = CHAT_MODEL) -> str:
    """
    Invoke the LLM and get a single, complete response.

    Args:
        prompt (str | List[dict]): The prompt or message history (can be a string or list of message dicts).
        model (str): Model name to use.

    Returns:
        str: The assistant's response or an error message.
    """
    try:
        llm = OllamaLLM(model=model, temperature=0.7)
        return llm.invoke(prompt)
    except Exception as e:
        return f"LLM Error: {e}"

def stream_llm(prompt: str, model: str = CHAT_MODEL) -> Iterator[str]:
    """
    Stream LLM response chunks, useful for real-time/interactive UIs.

    Args:
        prompt (str): The input prompt.
        model (str): Model name to use.

    Yields:
        Iterator[str]: Stream of response chunks (as they are generated by the model).
    """
    try:
        llm = OllamaLLM(model=model, temperature=0.7)
        return llm.stream(prompt)
    except Exception as e:
        # It's generally better for a stream to raise, but this will yield an error string.
        yield f"LLM Stream Error: {e}"
